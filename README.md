# cse151_project
Group Project Repository for CSE 151A

## [Link to dataset](https://www.google.com/url?q=https://www.kaggle.com/datasets/imranbukhari/comprehensive-ethusd-1m-data/data&sa=D&source=docs&ust=1730502587695059&usg=AOvVaw3kMqZe-yQhr2LT-L_PQyeM)

For our preprocessing, we will not need to impute any missing values because we found that there were no null values during our data exploration. The data is originally represented in 1 minute intervals with `Open`, `High`, `Low`, and `Close` prices, but there is very minimal change to be found within such a short interval. Thus, we condense our data into 10 minute intervals by manually calculating the corresponding price points. In addition, we filter our data to only contain rows from the year 2023 to limit the size of our dataset to fit github constraints. We then log transform the 'Volume' column because it is skewed and the scale of the data is larger than the other numerical volumes. This will make the data easier to analyze and prepare it for machine learning models. Since there are no categorical variables, we will not need to do any encoding.

In Milestone 3, we decided to save a snapshot and reduce our dataset to 300 thousand data points due to its constant increasing size by the daily addition of new data. After preprocessing, we created a model using linear regression and plotted our coefficients and intercepts to visualize our model's performance. Upon reviewing the graph and evaluation metrics, we answered the required questions, determined next steps to try with our model, and wrote a conclusion.

In Milestone 4, we first decided to go back and MinMax scale our data. We then reran our initial model from milestone 3 and implemented a DecisionTreeRegressor using RandomizedSearchCV to find optimal hyper parameters. Afterwards, we plotted the fitting graphs of the DecisionTreeRegressor's train and test errors at different parameter values for both min_samples_split and min_samples_leaf. Our new conclusions were then gathered at the end of our notebook.

Introduction:
We chose this Ethereum dataset because cryptocurrency represents a cutting-edge intersection of technology, finance, and societal change, making it highly relevant in the modern world. Ethereum, in particular, is not just a cryptocurrency but also a decentralized computing platform that enables the creation of smart contracts and decentralized applications (dApps). It has revolutionized industries such as finance, gaming, and supply chain management, offering secure, transparent, and efficient solutions to complex problems. Millions of individuals and organizations worldwide actively invest in and utilize Ethereum, making it a real-world asset with substantial economic, technological, and cultural significance.

What makes this project exciting is the opportunity to work with data from such a groundbreaking domain. Predicting Ethereum's price and performance is incredibly cool because it challenges us to explore and apply advanced machine learning techniques to a volatile and dynamic market. Additionally, the insights derived from analyzing Ethereum's historical data could have practical applications, such as guiding investment decisions, improving financial risk management, and even informing broader discussions about the adoption of blockchain technologies.

The broader impact of developing a robust predictive model for Ethereum extends far beyond individual projects. A good predictive model can help investors, companies, and even our group make informed decisions by providing accurate forecasts of market trends. It can improve portfolio management strategies, enhance algorithmic trading systems, and reduce financial risks associated with the volatile nature of cryptocurrency markets. Furthermore, these models can serve as valuable tools for educating people about the complexities of blockchain technology and encouraging its responsible adoption, ultimately contributing to the growth and stability of the digital economy.

In summary, working on Ethereum data is not only highly relevant and engaging but also offers the potential to contribute to real-world financial advancements and foster broader understanding and utilization of blockchain innovations.
